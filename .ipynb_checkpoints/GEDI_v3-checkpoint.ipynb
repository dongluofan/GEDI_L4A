{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d67300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "#import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import h5py\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.ops import orient\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715bffa",
   "metadata": {},
   "source": [
    "# 1 Load region to download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a88052",
   "metadata": {},
   "source": [
    "### 1.0 Func to load region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_region(filename,filepath=\"\",driver=\"shp\"):\n",
    "    if driver == \"shp\":\n",
    "        boundary = gpd.read_file(os.path.join(filepath,filename))\n",
    "        boundary = boundary.to_crs(epsg=4326)\n",
    "        boundary.geometry = boundary.geometry.apply(orient, args=(1,))\n",
    "        boundary = boundary.geometry.to_json()\n",
    "    elif driver=='GeoJSON':\n",
    "        with open(os.path.join(filepath,filename)) as f:\n",
    "            boundary = json.load(f)\n",
    "            boundary = json.dumps(boundary)\n",
    "    elif driver == 'ee':\n",
    "        try: \n",
    "            import ee\n",
    "            ee.Initialize()\n",
    "        except:\n",
    "            print('Fail to import ee')\n",
    "            return\n",
    "        AssetId = filename\n",
    "        region = ee.FeatureCollection(AssetId)\n",
    "        boundary = region.getInfo()\n",
    "        boundary = json.dumps(boundary)\n",
    "    return boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sea.geojson'\n",
    "driver = 'GeoJSON' # Driver could be shp, GeoJSON, or ee\n",
    "filepath = r'/projectnb/diprs/students/lfdong/GEDI'\n",
    "boundary = load_region(filename, filepath, driver) # Example for reading region from Shapefile\n",
    "# boundary = load_region(filename, filepath,'GeoJSON') # Example for reading region from geojson\n",
    "# boundary = load_region(assetId, \"\", 'ee') # Example for reading region from GEE\n",
    "print(boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5efdf0",
   "metadata": {},
   "source": [
    "# 2 Start searching file list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee7ab5",
   "metadata": {},
   "source": [
    "### 2.1 Func for searching files from NASA CMR API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(boundary, temporal_range, filename='granules.txt'):\n",
    "    doi = '10.3334/ORNLDAAC/2056'# GEDI L4A DOI \n",
    "\n",
    "    # CMR API base url\n",
    "    cmrurl='https://cmr.earthdata.nasa.gov/search/' \n",
    "\n",
    "    doisearch = cmrurl + 'collections.json?doi=' + doi\n",
    "    concept_id = requests.get(doisearch).json()['feed']['entry'][0]['id']\n",
    "\n",
    "    geojson = {\"shapefile\": (\"amapa.geojson\", boundary, \"application/geo+json\")}\n",
    "\n",
    "    page_num = 1\n",
    "    page_size = 2000 # CMR page size limit\n",
    "\n",
    "    dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    temporal_str = temporal_range['start'].strftime(dt_format) + ',' + temporal_range['stop'].strftime(dt_format)\n",
    "\n",
    "    granule_arr = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "         # defining parameters\n",
    "        cmr_param = {\n",
    "            \"collection_concept_id\": concept_id, \n",
    "            \"page_size\": page_size,\n",
    "            \"page_num\": page_num,\n",
    "            \"simplify-shapefile\": 'true', # this is needed to bypass 5000 coordinates limit of CMR,\n",
    "            \"temporal\": temporal_str,\n",
    "        }\n",
    "\n",
    "        granulesearch = cmrurl + 'granules.json'\n",
    "        response = requests.post(granulesearch, data=cmr_param, files=geojson)\n",
    "        granules = response.json()['feed']['entry']\n",
    "\n",
    "        if granules:\n",
    "            for g in granules:\n",
    "                granule_url = ''\n",
    "                granule_poly = ''\n",
    "\n",
    "                # read file size\n",
    "                granule_size = float(g['granule_size'])\n",
    "\n",
    "                # reading bounding geometries\n",
    "                if 'polygons' in g:\n",
    "                    polygons= g['polygons']\n",
    "                    multipolygons = []\n",
    "                    for poly in polygons:\n",
    "                        i=iter(poly[0].split (\" \"))\n",
    "                        ltln = list(map(\" \".join,zip(i,i)))\n",
    "                        multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
    "                    granule_poly = MultiPolygon(multipolygons)\n",
    "\n",
    "                # Get URL to HDF5 files\n",
    "                for links in g['links']:\n",
    "                    if 'title' in links and links['title'].startswith('Download') \\\n",
    "                    and links['title'].endswith('.h5'):\n",
    "                        granule_url = links['href']\n",
    "                granule_arr.append([granule_url, granule_size, granule_poly])\n",
    "\n",
    "            page_num += 1\n",
    "        else: \n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    # adding bound as the last row into the dataframe\n",
    "    # we will use this later in the plot\n",
    "    #granule_arr.append(['amapa', 0, amapa.geometry.item() ]) \n",
    "\n",
    "    # creating a pandas dataframe\n",
    "    l4adf = pd.DataFrame(granule_arr, columns=[\"granule_url\", \"granule_size\", \"granule_poly\"])\n",
    "\n",
    "    # Drop granules with empty geometry\n",
    "    l4adf = l4adf[l4adf['granule_poly'] != '']\n",
    "\n",
    "    print (\"Total granules found: \", len(l4adf.index)-1 )\n",
    "    print (\"Total file size (MB): \", l4adf['granule_size'].sum())\n",
    "\n",
    "    # drop duplicate URLs if any\n",
    "    l4a_granules = l4adf[:-1].drop_duplicates(subset=['granule_url'])\n",
    "    l4a_granules.to_csv(filename, columns = ['granule_url'], index=False, header = False)\n",
    "    return l4a_granules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0f62a",
   "metadata": {},
   "source": [
    "### 2.2 Set up temporal range and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6087626",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_range = {'start': datetime(2021, 1,1), \n",
    "                  'stop': datetime(2022, 1, 1)}\n",
    "filelist_df = get_files(boundary, temporal_range, filename='granules_sea.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f377bea",
   "metadata": {},
   "source": [
    "### 2.3 Show the first 5 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dc960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filelist_df['granule_url'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35460c7e",
   "metadata": {},
   "source": [
    "# 3 Start downloading files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bc8ed",
   "metadata": {},
   "source": [
    "### 3.0 Func for downloading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(df, taskname, output_path, username, password):\n",
    "    makedir(output_path)\n",
    "    current_path = os.getcwd()\n",
    "    if os.path.exists(taskname):\n",
    "        task = pd.read_csv(taskname, index_col=[0])\n",
    "    else:\n",
    "        task = pd.DataFrame([],columns=['granule_url','success'])\n",
    "    os.chdir(output_path)\n",
    "    for i in range(df.shape[0]):\n",
    "        url = df.loc[i,'granule_url']\n",
    "        downloading_filename = url.split('/')[-1]\n",
    "        if downloading_filename in list(task['granule_url']):\n",
    "            print(downloading_filename,'exists')\n",
    "            continue\n",
    "        else:\n",
    "            delete_files(output_path,downloading_filename)\n",
    "        try:\n",
    "            # use wget to download\n",
    "            !wget --http-user={username} --http-password={password} {url}\n",
    "            task.loc[task.shape[0]] = downloading_filename, 1\n",
    "            print(downloading_filename,'success')\n",
    "        except:\n",
    "            task.loc[task.shape[0]] = downloading_filename, 0\n",
    "            print(downloading_filename,' fail')\n",
    "        task.to_csv(taskname)\n",
    "    os.chdir(current_path)\n",
    "    \n",
    "def delete_files(path, file):\n",
    "    fileList = glob(os.path.join(path,file)+'*')\n",
    "    for filePath in fileList:\n",
    "        try:\n",
    "            os.remove(filePath)\n",
    "        except:\n",
    "            print(\"Error while deleting file : \", filePath)\n",
    "            \n",
    "\n",
    "def makedir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a890ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To record downloaded files,so when the downloading job is interupted, you could restart the downloading from \n",
    "# where you left and save some time in case the downloading is interupted. \n",
    "taskname = r'/projectnb/diprs/students/lfdong/GEDI/download_sea.csv' \n",
    "# Output_path for downloaded files\n",
    "# If the path doesn't exist, the download function will create the folder for you.\n",
    "output_path = r'/projectnb/diprs/students/lfdong/GEDI/data' \n",
    "username = 'YOUR USER NAME'  \n",
    "password = 'YOUR PASSWORD'\n",
    "download(filelist_df, taskname, output_path,username,password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56990ddd",
   "metadata": {},
   "source": [
    "# 4 Subset hdf5 files to csv to save some space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d329e99",
   "metadata": {},
   "source": [
    "### 4.0 Func for subsetting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2057a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubsetDf(subfile, var_list, beams):\n",
    "    full_power_beams = ['BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']\n",
    "    coverage_power_beams = ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011']\n",
    "    subset_df = pd.DataFrame()\n",
    "    hf = h5py.File(subfile, 'r')\n",
    "    for var in list(hf.keys()):\n",
    "        if var in beams:\n",
    "            beam = hf.get(var)\n",
    "            col_val = []\n",
    "            col_names = []\n",
    "            for key in var_list:\n",
    "                    col_names.append(key)\n",
    "                    #print(key)\n",
    "                    value = beam.get(key)[:]\n",
    "                    #print(key)\n",
    "                    col_val.append(value.tolist())\n",
    "            beam_df = pd.DataFrame(map(list, zip(*col_val)), columns=col_names)\n",
    "            beam_df['BEAM'] = var\n",
    "            subset_df = pd.concat([subset_df, beam_df])\n",
    "    \n",
    "    subset_df = subset_df[subset_df['l4_quality_flag']==1]\n",
    "    \n",
    "    start = datetime(2018,1,1,0,0)\n",
    "    \n",
    "    def time_stamp(delta_time):\n",
    "        date = start + timedelta(seconds=delta_time)\n",
    "        return int(date.timestamp()*1000)\n",
    "\n",
    "    if 'agbd_pi_lower' in var_list:\n",
    "        subset_df = subset_df.replace(-9999,0)\n",
    "\n",
    "    subset_df.delta_time = subset_df['delta_time'].apply(time_stamp)\n",
    "    subset_df.delta_time = subset_df.delta_time.astype('int64')\n",
    "    subset_df = subset_df.rename(columns={'delta_time':'system:time_start'})\n",
    "    def full_power(s):\n",
    "        if s in full_power_beams:\n",
    "            return 1\n",
    "        if s in coverage_power_beams:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    subset_df['full_power'] = subset_df['BEAM'].apply(full_power)\n",
    "    subset_df['fixed'] = -1 # to mark the fixing status of the file\n",
    "    return subset_df\n",
    "    \n",
    "def read_file(filename,columns):\n",
    "    if os.path.exists(filename):\n",
    "        task = pd.read_csv(filename, index_col=[0])\n",
    "    else:\n",
    "        task = pd.DataFrame([],columns=columns)\n",
    "    return task\n",
    "\n",
    "def makedir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45658d60",
   "metadata": {},
   "source": [
    "### 4.1 Subset h.5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd861c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'/projectnb/diprs/students/lfdong/GEDI/data' # path to all the .h5 files\n",
    "out_path = r'/projectnb/diprs/students/lfdong/GEDI/sea_subset_2021' # path you want to keep subsetted csv\n",
    "makedir(out_path)\n",
    "beams = ['BEAM0000', 'BEAM0001', 'BEAM0010', 'BEAM0011',\n",
    "         'BEAM0101', 'BEAM0110', 'BEAM1000', 'BEAM1011']\n",
    "var_list = ['agbd','l4_quality_flag','delta_time','lat_lowestmode','lon_lowestmode',\n",
    "            'agbd_pi_lower','agbd_pi_upper','agbd_se']\n",
    "out_var = ['agbd','system_index:start']\n",
    "file_list = os.listdir(path)\n",
    "for filename in file_list:\n",
    "    if os.path.exists(os.path.join(out_path,filename)):\n",
    "        print(filename,'exists')\n",
    "        continue\n",
    "    print(filename,'exporting')\n",
    "    df = getSubsetDf(os.path.join(path,filename), var_list, beams)\n",
    "    df.to_csv(os.path.join(out_path,filename+'.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd363b09",
   "metadata": {},
   "source": [
    "# 5 Merge subsetted files into biger files to upload to GEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646f0e2",
   "metadata": {},
   "source": [
    "### 5.0 Func for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(input_path, output_path, out_vars,offset=0, batch=200, reverse=False):\n",
    "    makedir(output_path)\n",
    "    filelist = os.listdir(input_path)\n",
    "    filelist.sort()\n",
    "    if reverse == True:\n",
    "        filelist.reverse()\n",
    "    N = len(filelist)\n",
    "    batch_list = [i for i in range(int(np.ceil(N/batch)))]\n",
    "    for i in batch_list:\n",
    "        print('Working on',i*batch,(i+1)*batch)\n",
    "        out_file = os.path.join(output_path, 'batchsize_%d_No_%.3d' % (batch,i+offset)+'.csv')\n",
    "        if os.path.exists(out_file):\n",
    "            print('File Exists:', out_file)\n",
    "            continue\n",
    "        temp_filelist = filelist[i*batch:(i+1)*batch]\n",
    "        df = pd.DataFrame([])\n",
    "        for j,file in enumerate(temp_filelist):\n",
    "            temp_df = pd.read_csv(os.path.join(input_path,file),index_col=[0])\n",
    "            if(temp_df.shape[0]==0):\n",
    "                continue\n",
    "            temp_df = df_to_geodf(temp_df)\n",
    "            #print(temp_df.head())\n",
    "            temp_df = temp_df[out_vars+['geometry']]\n",
    "            print('----------->',j,temp_df.shape[0])\n",
    "            df = pd.concat([df, temp_df],axis=0)\n",
    "        print('Merged dataFrame size: ',df.shape[0])\n",
    "        geodf_to_file(df, out_file, out_vars, driver='csv')\n",
    "\n",
    "def makedir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def df_to_geodf(df):\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon_lowestmode, df.lat_lowestmode))\n",
    "    gdf.crs = \"EPSG:4326\"  \n",
    "    return gdf\n",
    "\n",
    "def geodf_to_file(gdf, filename,var_list, driver='csv'):\n",
    "    gdf = gdf[var_list+['geometry']]\n",
    "    for c in gdf.columns:\n",
    "        if gdf[c].dtype == 'object':\n",
    "            gdf[c] = gdf[c].astype(str)\n",
    "    if driver == 'GeoJSON':\n",
    "        # Export to GeoJSON\n",
    "        gdf.to_file(filename, driver='GeoJSON')\n",
    "    if driver == 'shp':\n",
    "        # Export to ESRI Shapefile\n",
    "        gdf.to_file(filename)\n",
    "    if driver == 'csv':\n",
    "         # Export to CSV\n",
    "        gdf.to_csv(filename, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r'/projectnb/diprs/students/lfdong/GEDI/sea_subset'\n",
    "output_path = '/projectnb/diprs/students/lfdong/GEDI/sea_merge'\n",
    "out_vars = ['agbd','system:time_start','agbd_pi_lower','agbd_pi_upper','full_power']\n",
    "\n",
    "# how many files will be merged into one\n",
    "# there is a uplimit of the file size that could be upload to gee.\n",
    "# Here I recommand 200 to 400\n",
    "batch = 100 \n",
    "offset = 0 # Add a offset to filename. If offset = 100, the first file will be named as No_100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge(input_path, output_path, out_vars, offset, batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
